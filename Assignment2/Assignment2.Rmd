---
title: "Econometrics III - Assignment 2"
author: "The Explosive Invertibles"
date: "01 Apr 2022"
output:
  pdf_document:
    includes:
      in_header: "preamble.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(TSstudio)
library(stringr)
library(zoo)
library(ggplot2)
library(tseries)
library(stats)
library(lmtest)
library(Metrics)
library(dynlm)
library(dLagM)
library(tsoutliers)
library(DescTools)
library(naniar)
library(tseries)
library(dplyr)
library(apt)
library(tsDyn)
library(vars)
library(urca)
library(tidyverse)
```

# Question 1

```{r}
# Load data
data <- read.csv("data_assign_p3.csv")
```




```{r}
set.seed(1)
t = 100
n = 10000

sigma_u = 1
sigma_v = 1

# Create the first value
x <- data.frame(rnorm(n,0,sigma_v))
y <- data.frame(rnorm(n,0,sigma_u))

# Generate Random walk
for (i in 1:(t-1)){
v <- as.numeric(x[i,]) + rnorm(n,0,sigma_v)
x <- data.frame(x, v)

u <- as.numeric(y[i,]) + rnorm(n,0,sigma_u)
y <- data.frame(y, u)
}

# Run regressions & store beta, SE and R^2
est <- matrix(,nrow = n, ncol =3)

for (i in 1:n){
  reg <- summary(lm(as.numeric(y[i,])~ as.numeric(x[i,])))
  est[i,1] <- reg$coef[2,1]
  est[i,2] <- reg$coef[2,2]
  est[i,3] <- reg$r.squared
}

```

```{r}
# Plot histograms
plot(density(est[,1])$x,density(est[,1])$y,type="l",xlab="Distribution of estimated betas",ylab="Density")
plot(density(est[,2])$x,density(est[,2])$y,type="l",xlab="Distribution of estimated S.E.",ylab="Density")
plot(density(est[,3])$x,density(est[,3])$y,type="l",xlab="Distribution of R^2",ylab="Density")
```


2. Provide plots for two stock market time-series at your choice and report 12-period ACF
and PACF functions for those two time series. What does the sample ACF tell you about
the dynamic properties of these stocks?
```{r}
data$DATE<- as.Date(data$DATE, format = "%d/%m/%Y")
```

```{r}
ggplot(data, aes(x = DATE, y = NETFLIX)) + 
  geom_line() 
```
```{r}
ggplot(data, aes(x = DATE, y = APPLE)) + 
  geom_line() 
```

```{r}
Acf(
  data$NETFLIX,
  lag.max = 12
)
```

```{r}
Pacf(
  data$NETFLIX,
  lag.max = 12
)
```
```{r}
Acf(
  data$APPLE,
  lag.max = 12
)
```

```{r}
Pacf(
  data$APPLE,
  lag.max = 12
)
```

DON'T FORGET TO TALK ABOUT CONFIDENCE INTERVALS!

3. Perform an ADF unit-root test for all the 10 time series using the general-to-specific
approach based on the Schwarz Information Criterion (SIC). Report the values of the
ADF test statistics. Is the unit-root hypothesis rejected for any time-series at the 90%
confidence level? Did you expect to reject the unit-root hypothesis for some time-series at
this confidence level? Justify your answer carefully.

```{r}
# Set combinatorics where 1 last lag is intercept
max_lag = 6
lag <- c(1:max_lag)
comb_set <- CombSet(lag, m=1:max_lag)
comb = 0
for (i in 1:max_lag){
  comb = comb + CombN(max_lag,i)
}
lag_struc <- matrix(0,nrow = comb, ncol =max_lag)
lag_max <- rep(0, comb)

# Lag 1
lag_n = 1
for (i in 1:length(comb_set[[lag_n]])){
  lag = comb_set[[lag_n]][i]
  lag_struc[i,lag] = NA
}

# Lag 2 and higher
start = 0
for (n in 2:max_lag){
  lag_n = n
  start = start + length(comb_set[[lag_n-1]][,1])
  for (i in 1:length(comb_set[[lag_n]][,1])){
    for (j in 1:lag_n){
      lag = comb_set[[lag_n]][i,j]
      lag_struc[start + i,lag] = NA
    }
  }
}

# Delete model with only intercept
lag_struc <- lag_struc[-max_lag,]
comb = comb -1
```

```{r}
# Estimate all models and pick specification with lowest AIC - APPLE
ts <- data$APPLE
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta/se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with lowest AIC - EXXON_MOBIL
ts <- data$EXXON_MOBIL
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with lowest AIC - FORD
ts <- data$FORD
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with lowest AIC - GEN_ELECTRIC
ts <- data$GEN_ELECTRIC
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with lowest AIC - INTEL
ts <- data$INTEL
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with highest AIC - MICROSOFT
ts <- data$MICROSOFT
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with highest AIC - NETFLIX
ts <- data$NETFLIX
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with highest AIC - NOKIA
ts <- data$NOKIA
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with highest AIC - SP500
ts <- data$SP500
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with highest AIC - YAHOO
ts <- data$YAHOO
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta / se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```
4. Assume that both the stocks of Apple and Microsoft follow a random walk process. Produce
a 5-day forecast for the stocks of Apple and Microsoft. Add 95% confidence bounds to your
forecasts under the assumption of Gaussian innovations. Is there any investment advice
you can give on these stocks? Is their value expected to increase or decrease?

```{r}
# Estimate sigma for APPLE & MICROSOFT
sigma_aapl = sum(diff(data$APPLE)^2) / (length(diff(data$APPLE)))
sigma_msft = sum(diff(data$MICROSOFT)^2) / (length(diff(data$MICROSOFT)))

# Store last h observations for each stock price
t = 20
hist_aapl <- tail(data$APPLE,t)
hist_msft <- tail(data$MICROSOFT,t)

# Produce forecast
h = 5
forecast_aapl <-  rep( tail(data$APPLE,1),h)
forecast_msft <- rep( tail(data$MICROSOFT,1),h)
forecast_aapl <-  ts(forecast_aapl, start = t+1)
forecast_msft <- ts(forecast_msft, start = t+1)
```

```{r}
# Generate plot APPLE
ts.plot(forecast_aapl)
points(forecast_aapl - 1.96*sqrt(sigma_aapl), type="l", col=2, lty=2)
points(forecast_aapl + 1.96*sqrt(sigma_aapl), type="l", col=2, lty=2)
```
```{r}
# Generate plot MICROSOFT
ts.plot(forecast_msft)
points(forecast_msft - 1.96*sqrt(sigma_msft), type="l", col=2, lty=2)
points(forecast_msft + 1.96*sqrt(sigma_msft), type="l", col=2, lty=2)
```
5. Do you find a statistically significant contemporaneous relation between Microsoft and
Exxon Mobile stock prices? Do you agree that changes in Microsoft stock prices are
largely explained by fluctuations in the stock price of Exxon Mobile? Justify your answer.

```{r}
mod <- lm(data$APPLE ~ data$EXXON_MOBIL)
summary(mod)
```
SPURIOUS REGRESSION..


## Question 2

1. Show that the spurious regression problem does not occur when two I(1) variables are
cointegrated using a Monte Carlo simulation study. Furthermore, analyze the claim of
superconsistency of the estimator of the static cointegrating regression. 

```{r}
# Simulation with x being non-stationary
set.seed(20)
l = c(50, 100)
n = 10000

sigma_u = 1
sigma_v = 1

gamma = 0.8
phi = 1

# Create the list to store dataframes
simulationsX <- list()
simulationsY <- list()


# Generate Random walk
for (j in 1:length(l)){
  t = l[j]
  x <- data.frame(rnorm(n,0,sigma_v))
  y <- data.frame(rnorm(n,0,sigma_u))
  for (i in 1:(t-1)){
    v <- as.numeric(x[i,]) + phi * rnorm(n,0,sigma_v)
    x <- data.frame(x, v)
    
    u <- as.numeric(x[(i+1),]) + gamma * rnorm(n,0,sigma_u)
    y <- data.frame(y, u)
  }
  simulationsX[j] <- list(data.frame(x))
  simulationsY[j] <- list(data.frame(y))
}

# Run regressions & store beta, SE and R^2
list_est <- list()

for (j in 1:length(l)){
  est <- data.frame(matrix(ncol = 3, nrow = n))
  for (i in 1:n){
    reg <- summary(lm(as.numeric(simulationsY[[j]][i,])~ as.numeric(simulationsX[[j]][i,])))
    est[i,1] <- reg$coef[2,1]
    est[i,2] <- reg$coef[2,2]
    est[i,3] <- reg$r.squared
  }
  list_est[j] <- list(data.frame(est))
}

```

```{r}
# Simulation with x being stationary
set.seed(20)
l = c(50, 100)
n = 10000

sigma_u = 1
sigma_v = 1

gamma = 0.8
phi = 0.8

# Create the list to store dataframes
simulationsX_2 <- list()
simulationsY_2 <- list()


# Generate Random walk
for (j in 1:length(l)){
  t = l[j]
  x <- data.frame(rnorm(n,0,sigma_v))
  y <- data.frame(rnorm(n,0,sigma_u))
  for (i in 1:(t-1)){
    v <- as.numeric(x[i,]) + phi * rnorm(n,0,sigma_v)
    x <- data.frame(x, v)
    
    u <- as.numeric(x[(i+1),]) + gamma * rnorm(n,0,sigma_u)
    y <- data.frame(y, u)
  }
  simulationsX_2[j] <- list(data.frame(x))
  simulationsY_2[j] <- list(data.frame(y))
}

# Run regressions & store beta, SE and R^2
list2_est <- list()

for (j in 1:length(l)){
  est <- data.frame(matrix(ncol = 3, nrow = n))
  for (i in 1:n){
    reg <- summary(lm(as.numeric(simulationsY_2[[j]][i,])~ as.numeric(simulationsX_2[[j]][i,])))
    est[i,1] <- reg$coef[2,1]
    est[i,2] <- reg$coef[2,2]
    est[i,3] <- reg$r.squared
  }
  list2_est[j] <- list(data.frame(est))
}

```

```{r}
# Plot histograms
plot(density(est[,1])$x,density(est[,1])$y,type="l",xlab="Distribution of estimated betas",ylab="Density")
plot(density(est[,2])$x,density(est[,2])$y,type="l",xlab="Distribution of estimated S.E.",ylab="Density")
plot(density(est[,3])$x,density(est[,3])$y,type="l",xlab="Distribution of R^2",ylab="Density")
```

# muss noch guten vergleich finden

2. Plot both the aggregate consumption and aggregate income time-series (these series are
called cons and inc respectively in the csv file). Compute and report 12-period ACF and
PACF functions for each series and comment on their shape

```{r}
# Load data
data <- read.csv("data_assign_p4.csv")
```

```{r}
fmt <- "%YQ%q"
data$obs <- as.yearqtr(data$obs, format = fmt)
```


```{r}
# Plot of consumption
ggplot(data, aes(obs, CONS)) + 
  geom_point() + 
  geom_line() +
  scale_x_yearqtr(format = fmt)
```

```{r}
# Plot of income
ggplot(data, aes(obs, INC)) + 
  geom_point() + 
  geom_line() +
  scale_x_yearqtr(format = fmt)
```

```{r}
Acf(
  data$CONS,
  lag.max = 12
)
```

```{r}
Pacf(
  data$CONS,
  lag.max = 12
)
```
```{r}
Acf(
  data$INC,
  lag.max = 12
)
```

```{r}
Pacf(
  data$INC,
  lag.max = 12
)
```

3. Perform an ADF unit-root test on each series using the general-to-specific approach and
report the values of the test statistics. Is the unit-root hypothesis rejected in any of them?
```{r}
# Set combinatorics where 1 last lag is intercept
max_lag = 6
lag <- c(1:max_lag)
comb_set <- CombSet(lag, m=1:max_lag)
comb = 0
for (i in 1:max_lag){
  comb = comb + CombN(max_lag,i)
}
lag_struc <- matrix(0,nrow = comb, ncol =max_lag)
lag_max <- rep(0, comb)

# Lag 1
lag_n = 1
for (i in 1:length(comb_set[[lag_n]])){
  lag = comb_set[[lag_n]][i]
  lag_struc[i,lag] = NA
}

# Lag 2 and higher
start = 0
for (n in 2:max_lag){
  lag_n = n
  start = start + length(comb_set[[lag_n-1]][,1])
  for (i in 1:length(comb_set[[lag_n]][,1])){
    for (j in 1:lag_n){
      lag = comb_set[[lag_n]][i,j]
      lag_struc[start + i,lag] = NA
    }
  }
}

# Delete model with only intercept
lag_struc <- lag_struc[-max_lag,]
comb = comb -1
```

```{r}
# Estimate all models and pick specification with lowest AIC - CONS
ts <- data$CONS
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta/se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

```{r}
# Estimate all models and pick specification with lowest AIC - INC
ts <- data$INC
aics <- rep(0, (comb-1))

for (i in 1:(comb-1)){ # there is an issue with (1,1,1,1,1,1,)
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta/se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

# check out problem 

4. Perform an ADF unit-root test on the first difference of each series using the general-tospecific approach and report the values of the test statistics. Is the unit-root hypothesis
rejected in any of them? What do you conclude about the order of integration of these
time series?

```{r}
# Calculate first differences of income and consumption data
data2 <- data %>%
  mutate_at(vars(INC:CONS), list(~ .x - lag(.x)))
```

```{r}
# Estimate all models and pick specification with lowest AIC - CONS
ts <- data2$CONS
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta/se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```
```{r}
# Estimate all models and pick specification with lowest AIC - INC
ts <- data2$INC
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta/se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```

5. Assuming both series are I(1), test for cointegration between consumption and income by
regressing consumption on income and performing a unit-root test on the residuals. Report
the estimated regression coefficients. Plot the regression residuals. Use the Schwartz
Information Criterion (SIC) to determine the number of ADF lags in your unit-root residual
test. Report the cointegration test statistic. Do you reject cointegration?

```{r}
# Regress Consumption on Income to get get estimated regression coefficients
test <- lm(data$CONS ~ data$INC)
lambda <- test$coefficients[2]
delta <- test$coefficients[1]
lambda
delta
```

```{r}
# Calculate regression residuals
data$Z <- data$CONS - delta - lambda*data$INC
```
```{r}
# Plot regression residuals
ggplot(data, aes(obs, Z)) + 
  geom_point() + 
  scale_x_yearqtr(format = fmt)
```
```{r}
# Estimate all models and pick specification with lowest AIC - INC
ts <- data$Z
aics <- rep(0, comb)

for (i in 1:comb){
  logl <- arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[i,], method ="CSS")$loglik
  k = sum(is.na(lag_struc[i,]))
  aics[i] = 2*k - 2*logl
}
min(aics)
arima(x=ts, order = c(max_lag -1,0,0), fixed = lag_struc[which.min(aics),], method ="CSS")

# Perform ADF Test
ts <- ts(ts)
struc <- c(0)
for (i in 1:length(lag_struc[which.min(aics),]-1)){
  if (is.na(lag_struc[which.min(aics),i])){
    struc <- cbind(struc, i)
  } 
}
struc <- as.numeric(struc[,-1])
struc <- head(struc,-1)
if(is.na(lag_struc[which.min(aics),i])){
  int = TRUE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc))
} else{
  int = FALSE
  mod <- dynlm(diff(ts) ~ L(ts,1) + L(diff(ts),struc)-1)
}
beta = summary(mod)$coefficients[1,1]
se = summary(mod)$coefficients[1,2]
adf = beta/se
adf

if(int==TRUE){
  adf< (-1.616)
}else{
  adf< (-2.568)
}
```
As H0 is rejected, we have cointegration.

6. Estimate an error correction model for consumption using the estimated residuals from
the cointegration regression above. Use a general-to-specific modeling approach for the
short-run dynamics. Report the estimated model. Report and interpret the short-run and
long-run multipliers. Report and interpret the error correction coefficient.

```{r}
# Transform data into times seires format
tsCONS <- as.numeric(ts(data$CONS, start= c(1988, 1), end = c(2012, 1), frequency = 4))
tsINC <- as.numeric(ts(data$INC, start= c(1988, 1), end = c(2012, 1), frequency = 4))
dset <- cbind(tsCONS, tsINC)
```

```{r}
#Selecting the Optimal Number of Lags (Recall, this is p - 1)
lagselect <- VARselect(dset, lag.max = 7, type = "const")
lagselect$selection
lagselect$criteria
```
```{r}
# use one lag according to SIC (2-1 = 1), and co-integration order of 1 (from previous subquestion)
# estimate error correction model
ecm <- VECM(dset, 1, r = 1, estim =("2OLS"))
summary(ecm)
```
8. At the peak of the recession, during the 2nd quarter of 2009, you are asked to forecast the
value of consumption for the 3rd quarter of 2009. Do you expect aggregate consumption
to raise or fall? Report the predicted change in the value of consumption. How much of
this change in consumption is due to the ‘correction mechanism’ alone? Report your point
forecast for consumption for the 3rd quarter of 2009.

```{r}
forecast1 <- predict(ecm, n.ahead=2)
forecast1
```
# this should be right

```{r}
# Estimate an AR model for first difference of income
arINC <- arima(data2$INC, order = c(4,0,0))
arINC <- coeftest(arINC)
```
```{r}
# Forecast first differences of income based on estimated AR model
h = 2
INC <- append(rep(data2$INC[n],1), numeric(h))

for (i in 1:h){
  j = i + 1
  INC[j] <- arINC[5,1] + arINC[1,1]*INC[j-1]
}
```


```{r}
# Forecast first differences of consumption based on ECM model and forecasted values of first differences of income
h = 2
CONS <- append(rep(data2$CONS[n],1), numeric(h))
Z <- append(rep(data$Z[n],2), numeric(h))

for (i in 1:h){
  j = i + 1
  CONS[j] <- ecm$coefficients[1,1]*Z[j-1] + ecm$coefficients[1,2] + ecm$coefficients[1,4]*INC[j]
}
```

```{r}
# Forecast first differences of consumption based on ECM model and forecasted values of first differences of income
h = 2
CONS2 <- append(rep(data2$CONS[n],1), numeric(h))
Z <- append(rep(data$Z[n],2), numeric(h))

for (i in 1:h){
  j = i + 1
  CONS2[j] <- (ecm$coefficients[1,1]/(1-ecm$coefficients[1,3]))*Z[j-1] + (ecm$coefficients[1,2]/(1-ecm$coefficients[1,3])) + (ecm$coefficients[1,4]/(1-ecm$coefficients[1,3]))*INC[j]
}
```

# welche der beiden ist richtig????